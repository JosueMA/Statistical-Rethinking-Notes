---
title: "Chapter 12 Notes"
author: "Tim"
date: "10/22/2017"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, 
                      results='show', cache=TRUE, autodep=TRUE)
```

## 12.1 Multilevel Tadpoles

```{r}
library(rethinking)
library(brms)
library(tidyverse)
library(tidybayes)

rstan_options (auto_write=TRUE)
options (mc.cores=parallel::detectCores ()) # Run on multiple cores

data(reedfrogs)
d <- reedfrogs
d$tank <- 1:NROW(d)
d %>% as.tibble()
```

To fit the multilevel model in `brms` as described in we need to explicitly remove the population parameter with `-1` as shown below:

```{r, results='hide'}
mod.intercept <- brm(surv | trials(density) ~ -1 + (1 | tank),
                     family = binomial(), data=d,
                     prior = c(set_prior("normal(0,1)", class = 'sd',
                                         group = 'tank', coef='Intercept'),
                               set_prior("cauchy(0,1)", class = 'sd',
                                         group = 'tank')))
```
```{r}
summary(mod.intercept)
```

How do we interpret the summary of this hierarchical model? Well, in terms of the rethinking model on page 359, `sd(Intercept)` is the adaptive standard deviation from the Normal distribution that each tank intercept is draw from. In other words, each intercept is draw from a normal distribution with `sd(Intercept)` standard deviation with an adaptive mean. 

Let's reconstruct the plot on page 361:

```{r}
p <- ggplot(d, aes(x=tank)) +
  geom_point(aes(y=propsurv)) +
  geom_hline(yintercept = mean(d$propsurv), linetype=2) +
  geom_vline(xintercept = c(16.5, 32.5))
```

Now let's take a look at the predicted survival rates:

```{r}
d.mean <- d %>%
  add_fitted_samples(mod.intercept) %>%
  mean_qi() %>%
  mutate(propsurv_pred = estimate/density)
```

`brms` doesn't give the adaptive population prior $\alpha$ directly. We need to calculate it by sampling from the posterior parameters of the group-level intercepts. The function `tidybayes::spread_samples` is nice tool to easily sample parameters from the posterior into tidy data frames. `r_tank` is the name of the group-level intercepts. There is a standard convention to `brms` name, but you can always find the names with `parnames` as shown below. `tidybayes::spread_samples` is flexible enough to allow syntax matching for the parameter of interest. 

```{r}
parnames(mod.intercept)

# group parameter samples using tidybayes
pop.intercept <- mod.intercept %>% spread_samples(r_tank[tank,])
pop.proportion <- logistic(mean(pop.intercept$r_tank))

p +
  geom_point(aes(y=propsurv_pred), data=d.mean, shape=1) +
  geom_hline(yintercept = pop.proportion) # predicted population mean (intercept)
```

Here we can see the classic shrinkage on each tank using a multilevel model. Moreover the tanks are sorted from smallest to larger: therefore as we move to the right we have less shrinkage. Lastly, also note that the new estimated population mean is different than the observed mean.

As an aside, let's try to fit the model with a predictor. McElreath withholds varying slopes until the Chapter 13, so let's try a population predictor. I would expect that predators would decrease the the probability of survival.

```{r, results='hide'}
# indicator variable for predator
d$pred <- ifelse(d$pred == 'pred', 1, 0)
mod.pred <- brm(surv | trials(density) ~ pred + (1 | tank), data=d,
                family = binomial(),
                prior = c(set_prior("normal(0,1)", class = 'sd',
                                     group = 'tank', coef='Intercept'),
                           set_prior("cauchy(0,1)", class = 'sd',
                                     group = 'tank')))
```
```{r}
summary(mod.pred)
```

As expected, the presence of a predator has a strong effect on the survival of tadpoles.

It would also makes sense that survival depends on the number of tadpoles relative to the size of a tank, whether a predator is present or not. If there are many tadpoles in a small tank, the survival rate should decrease when a predator is present.

```{r, results='hide'}
mod.interaction <- brm(surv | trials(density) ~ pred*density*size
                       + (1 | tank),
                       data=d, family = binomial(),
                       prior = c(set_prior("normal(0,1)", class = 'sd',
                                     group = 'tank', coef='Intercept'),
                                 set_prior("cauchy(0,1)", class = 'sd',
                                     group = 'tank')))
```
```{r}
summary(mod.interaction)
```
```{r}
LOO(mod.intercept, mod.pred, mod.interaction)
```

Based on the LOO information criteria, the predator predictor model seems like the best fit. 

We can also do some posterior predictor checks on the response density, as advocated by Gelman et al in Chapter 6 of Bayesian Data Analysis. `pp_check` is a method in `brms` that calls the package `bayesplot`, a tie in Stan for visualizing the posterior:

```{r}
pp_check(mod.pred)
pp_check(mod.intercept)
```

A standard warning with these checks, as noted in the `pp_check` `brms` documentation, a graphical fit may look good for both models. Indeed, here both seem to fit okay, with `mod.pred` being a bit better as expected. Information criteria like LOO help us select the model in light of clear graphical errors.

## Multilevel chimps

Next we return to the chimp data and consider multiple cluster types.

```{r}
data("chimpanzees")
d <- chimpanzees
d$recipient <- NULL
d %>% as.tibble()
```

#### One Cluster

First, we'll fit one cluster:

```{r, results='hide'}
mod <- brm( pulled_left ~ 1 + (1 | actor) +
                      prosoc_left*condition - condition,
                    data = d, family = bernoulli(), iter = 5000,
                    prior = c(set_prior("normal(0,10)", class = 'Intercept'),
                              set_prior("normal(0,10)", class = 'b'),
                              set_prior("cauchy(0,1)", class = 'sd',
                                        group = 'actor')))
```
```{r}
summary(mod)
```

And to get the get the total intercepts for each actor as per R Code 12.22, we use `brms:coef`, which is the sum of the population and group level effects per level.

```{r}
coef(mod)$actor[,,'Intercept']
```

Alternatively, we can use `tidybayes`. One reason to prefer `tidybayes` is that it has consistent `tidyverse` style syntax and always outputs tidy tibbles, grouped by `spread_sample` parameters for quick summaries.

```{r}
mod %>% 
  spread_samples(r_actor[actor,], b_Intercept) %>%
  mean_qi(r_actor + b_Intercept) # no group_by necessary, already included
```

#### Two Clusters

The study was organized into different blocks, where each monkey pulled their levels once per day as opposed to one monkey doing all their pulls at once. This technique called cross-classification is a useful design feature to eliminate temporal effects on the treatment.

Thus we can also provide unique intercepts for each blocks. Ideally, we want to see that there is little to no variation within each blot: that's the entire design goal of the blocks. If there is added variation in different blocks, we can measure that variation and see if the treatment appears after controlling for the block variation.

```{r, results='hide'}
mod.cluster <- brm(pulled_left ~ 1 + (1 | actor) + (1 | block) + 
                     prosoc_left + prosoc_left:condition,
                   data=d, family=bernoulli(),
                   prior = c(set_prior("normal(0,10)", class = 'Intercept'),
                             set_prior("normal(0,10)", class = 'b'),
                             set_prior("cauchy(0,1)", class = 'sd',
                                       group = 'actor')))
```
```{r}
summary(mod.cluster)
```
These results match the output of R Code 12.24. 

For the charter in Figure 12.4:

```{r}
# it would be nice if spread_samples() with no args just spread every
# parameter available. 
parnames(mod.cluster)
par_samples <- mod.cluster %>%
  gather_samples(r_actor[actor,], r_block[block,],
                 b_Intercept, b_prosoc_left, `b_prosoc_left:condition`,
                 sd_block__Intercept, sd_actor__Intercept) %>%
  replace_na(list(actor = "", block = "")) %>%
  unite(variable, term, actor, block)

par_samples %>%
  group_by(variable) %>%
  mean_qi(estimate) %>%
  ggplot(aes(y = variable, x = estimate)) +
  geom_point() +
  geom_segment(aes(x=conf.low, xend=conf.high, yend=variable))
```

However, each parameter has a full marginal posterior distribution; this just shows the intervals. I'll introduce a few other ways to visualize these.

First we'll try the `tidybayes` plot `geom_halfeyeh`:
```{r, fig.align='center'}
ggplot(par_samples, aes(y=variable, x = estimate)) + 
  geom_halfeyeh()
```

Next, there is an intriguing package `ggridges` that is able to overlap density plots, so we see the entire parameter distribution with effective use of space.

```{r}
library(ggridges)
ggplot(par_samples, aes(y=variable, x = estimate)) + 
  geom_density_ridges()
```



Lastly, let's compare the fixed effects model:

```{r, results='hide'}
mod.fixed <- brm(pulled_left ~ actor + block + prosoc_left + prosoc_left:condition,
                 data = d, family=bernoulli(),
                 prior = c(set_prior("normal(0,10)", class="Intercept"),
                           set_prior("normal(0,10)", class='b')))
```
```{r}
summary(mod.fixed)
```



And the LOO comparison:

```{r}
LOO(mod, mod.cluster, mod.fixed)
```

Unsurprisingly, the fixed effect LOOIC is much worse, taking into account the standard error. 

Page 376 has a good closing note on model comparison with information criteria. Rather than 'selecting' models, we can use LOO/WAIC/etc. as a way to explain the data/phenomenon. The comparison, not the 'selection' tells us that the inclusion of block doesn't matter, and the small standard deviation of block intercepts tell us why. This is something we can miss out on if we just present a fitted model with only actor intercepts and assure the readers that block doesn't matter.

## Posterior Predictors within clusters

A word of warning with multilevel models. The posterior predictive checks are going to look different than the raw data due to the inherent shrinkage. We'll see that in the next few examples. 

The outlier 2 versus actor 3, representative of other actors. 

```{r}
library(modelr)
d.pred <- d %>%
  data_grid(prosoc_left = c(0,1),
            condition = c(0,1),
            actor = c(2,3)) %>%
  add_fitted_samples(mod) %>%
  mean_qi() %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep = "/")
```
```{r}
ggplot(d.pred, aes(x = prosoc_left_condition, y = estimate,
                   color=actor, group=actor)) +
  coord_cartesian(ylim = c(0, 1)) + 
  geom_line() +
  geom_ribbon(aes(ymin=conf.low, ymax=conf.high),
              alpha = 0.4, fill='grey60', color=NA)
  
```

All actors:

```{r}
d %>%
  data_grid(prosoc_left = c(0,1),
            condition = c(0,1),
            actor = 1:7) %>%
  add_fitted_samples(mod) %>%
  mean_qi() %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep = "/") %>%
  ggplot(aes(x = prosoc_left_condition, y = estimate, group=actor,
             color = as.factor(actor))) + 
    coord_cartesian(ylim = c(0, 1)) + 
    geom_line()
```

We see the expected behavior, that all chimps pull left when food is available, regardless if the another chimp is there to benefit. 

Let's take a look at some other graphical summaries of the actors.

One way to is to look at the response averaged over actors:

```{r}
d.pred <- d %>% 
  data_grid(prosoc_left = c(0,1),
            condition = c(0,1),
            actor = 1:7) %>% 
  add_fitted_samples(mod)
```
```{r}
d.pred %>%
  group_by(prosoc_left, condition) %>%
  mean_qi() %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep="/") %>%
  ggplot(aes(x = prosoc_left_condition, y=estimate, group=actor)) +
  geom_line() + 
  geom_ribbon(aes(ymin = estimate.low, ymax=estimate.high),
              alpha=0.4, fill='grey60') + 
  coord_cartesian(ylim = c(0, 1))
```

However, the intervals make the variation hard to see. Let's just directly sample the simulated actors. This is 50 samples for each actor. 

```{r}
d %>% 
  data_grid(prosoc_left = c(0,1),
            condition = c(0,1),
            actor = 1:7) %>% 
  add_fitted_samples(mod, n=50) %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep="/") %>%
  ggplot(aes(x = prosoc_left_condition, y=estimate, 
             group=interaction(.iteration, actor))) +
  geom_line(alpha = 0.25, color = 'red') +
  coord_cartesian(ylim = c(0, 1))
```

Here the lines are a really effective presentation because it shows that the mean is slightly misleading us. The distribution isn't uniform; most of the clusters samples appear to be below `0.50`, where the mean is estimated. Let's try the previous plot again, but with the median:

```{r}
d.pred %>%
  group_by(prosoc_left, condition) %>%
  median_qi() %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep="/") %>%
  ggplot(aes(x = prosoc_left_condition, y=estimate, group=actor)) +
  geom_line() + 
  geom_ribbon(aes(ymin = estimate.low, ymax=estimate.high),
              alpha=0.4, fill='grey60') + 
  coord_cartesian(ylim = c(0, 1))
```

This confirms our theory: the sampling isn't uniformly distributed across the probability scale. Most of the observations are lower, implying they pull left less, but there is enough variation/outliers (actors who always pull left) that the mean is pulled way up compared to the median. 


## Posterior Predictions for new clusters

The previous section didn't demonstrate any new tricks. But now we want to predict for new clusters. In the model, the clusters are seven individual chimps. But from those 7, we were able to estimate the variation or distribution in the chimp population. Therefore, we will use `sd(Intercept)` to make those inferences.

So we want to sample the intercept for a new actor from a normal distribution with mean zero and a standard deviation of `sd(Intercept)`.

First, using the `brms` function parameter `re_formula`, we we same an arbitrary actor and tell the model to make predictions based solely off the population intercept and predictors.

As a warning: all confidence intervals will be 80% to be consistent with the graphs in figure 12.5, page 380:

```{r}
d.pred <- d %>% 
  data_grid(prosoc_left = c(0,1),
            condition = c(0,1),
            actor = 1) %>% 
  add_fitted_samples(mod, re_formula = NA) %>% 
  group_by(prosoc_left, condition) %>% 
  mean_qi(.prob = .8) %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep="/")
d.pred %>%
  ggplot(aes(x = prosoc_left_condition, y=estimate, group=actor)) +
  geom_line() +
  geom_ribbon(aes(ymin = estimate.low, ymax=estimate.high),
              alpha=0.4, fill='grey60') +
  coord_cartesian(ylim = c(0, 1))
```

Next, we want to actually sample a new actor. Using the same approach described on page 379, we create a new actor and sample their individual intercept from a normal distribution with mean equal to zero and a standard deviation of `sd(Intercept)`:

```{r}
d.pred <- d %>% 
  data_grid(prosoc_left = c(0,1),
            condition = c(0,1),
            actor = 8) # a new individual, the number doesn't matter
d.pred %>%
  add_fitted_samples(mod, allow_new_levels = TRUE,
                     sample_new_levels = 'gaussian') %>%
  group_by(prosoc_left, condition) %>% 
  mean_qi(.prob = .8) %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep="/") %>%
  ggplot(aes(x = prosoc_left_condition, y=estimate, group=actor)) +
  geom_line() +
  geom_ribbon(aes(ymin = estimate.low, ymax=estimate.high),
              alpha=0.4, fill='grey60') +
  coord_cartesian(ylim = c(0, 1))
```

And since the confidence interval is so wide, let's try sampling individuals:

```{r}
d.pred %>%
  add_fitted_samples(mod, n=50, allow_new_levels = TRUE,
                     sample_new_levels = 'gaussian') %>%
  unite(prosoc_left_condition, prosoc_left, condition, sep="/") %>%
  ggplot(aes(x = prosoc_left_condition, y=estimate, 
             group=interaction(.iteration, actor))) +
  geom_line(alpha = 0.25, color = 'red') +
  coord_cartesian(ylim = c(0, 1))
```

These look pretty similar, up to ordering, of the graphs on 380. Let's make the `rethinking` model and compare our results.

First, the marginal:

```{r, results='hide'}
m12.4 <- map2stan(
  alist(
    pulled_left ~ dbinom(1,p),
    logit(p) <- a + a_actor[actor] + (bp + bpC * condition)*prosoc_left,
    a_actor[actor] ~ dnorm(0,sigma_actor),
    a ~ dnorm(0,10),
    bp ~ dnorm(0,10),
    bpC ~ dnorm(0,10),
    sigma_actor ~ dcauchy(0,1)
  ),
  data=d, warmup=1000, iter=5000, chains=4, cores=4)
```
```{r}
precis(m12.4)
```


```{r}
post <- extract.samples(m12.4)
post$a_actor_sim <- rnorm(16000, 0, post$sigma_actor)

linear_link <- function(prosoc_left, condition) {
  logodds <- with(post, 
                  a + a_actor_sim + bp * prosoc_left + bpC * prosoc_left * condition)
  return(logistic(logodds))
}

prosoc_left <- c(0,1,0,1)
condition <-   c(0,0,1,1)
pred.table <- sapply(1:4, function(i) 
  linear_link(prosoc_left[i], condition[i]))
colnames(pred.table) <- c("0/0", "1/0", "0/1", "1/1")

pred.table <- as.tibble(pred.table) %>% 
  gather(variable, estimate) %>%
  group_by(variable) %>%
  mean_qi(estimate, .prob = 0.8)
pred.table

ggplot(pred.table, aes(x = variable, y=estimate, group=.prob)) +
  geom_line() + 
  geom_ribbon(aes(ymin = conf.low, ymax=conf.high), alpha=0.4, fill='grey60') +
  coord_cartesian(ylim = c(0, 1))
```

Which is the exact sample plot (and table value) as the marginal plot used in `brms`.

Next, the average:

```{r}
linear_link <- function(prosoc_left, condition) {
  logodds <- with(post, 
                  a + bp * prosoc_left + bpC * prosoc_left * condition)
  return(logistic(logodds))
}

prosoc_left <- c(0,1,0,1)
condition <-   c(0,0,1,1)
pred.table <- sapply(1:4, function(i) 
  linear_link(prosoc_left[i], condition[i]))
colnames(pred.table) <- c("0/0", "1/0", "0/1", "1/1")
```
```{r}
pred.table <- as.tibble(pred.table) %>% 
  gather(variable, estimate) %>%
  group_by(variable) %>%
  mean_qi(estimate, .prob = 0.8)
pred.table

ggplot(pred.table, aes(x = variable, y=estimate, group=.prob)) +
  geom_line() + 
  geom_ribbon(aes(ymin = conf.low, ymax=conf.high), alpha=0.4, fill='grey60') +
  coord_cartesian(ylim = c(0, 1))
```

And we get the same results. I think the main problem for me was that the average actor plot looked like it peaked at `0.8`, but I think that is just an illusion. 

## Predicting new clusters with Over-dispersion in Oceanic societies

```{r}
data(Kline)
d <- Kline
d$logpop <- log(d$population)
d$society <- 1:10
d %>% as.tibble()
```
```{r, results='hide'}
mod <- brm(total_tools ~ 1 + (1 | society) + logpop,
             data = d, family=poisson(),
             prior = c(prior(normal(0,10), class = Intercept),
                       prior(normal(0,1), class = b),
                       prior(cauchy(0,1), class = sd, group = society)))
```
```{r}
summary(mod)
```

```{r}
d %>% 
  data_grid(logpop = seq_range(logpop, n=10),
            society = 11) %>%
  add_predicted_samples(mod, allow_new_levels=TRUE,
                        sample_new_levels='gaussian') %>%
  mean_qi() %>%
  ggplot(aes(x=logpop, y=pred)) + 
  geom_line(aes(group=society)) +
  geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha=0.4, color='grey60') + 
  geom_point(data=d, aes(x=logpop, y=total_tools))
```
```{r}
d %>% 
  data_grid(logpop = seq_range(logpop, n=10),
            society = 11) %>%
  add_predicted_samples(mod, allow_new_levels=TRUE,
                        sample_new_levels='gaussian') %>%
  ggplot(aes(x=logpop)) +
  stat_lineribbon(aes(y=pred), .prob = c(0.95, 0.75, 0.5)) +
  geom_point(data=d, aes(x=logpop, y=total_tools)) +
  scale_fill_brewer()
  
```

